{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNP5yZgSGtA7JEkWEOo8hHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manmeet3/Deep_Learning2/blob/master/Asg4/meta-learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxFvxeG6FvOw"
      },
      "source": [
        "## Meta Learning\n",
        "Using Omni glot dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBeP3iJPF6w3"
      },
      "source": [
        "# ref: https://github.com/shashankhalo7/Omniglot_meta_learning/blob/master/osl.ipynb"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsRdGrosgM_8",
        "outputId": "646acb14-4607-4b6d-987e-a2e6603f4ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!wget https://github.com/manmeet3/Deep_Learning2/raw/master/Asg4/images_background.zip"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 05:03:00--  https://github.com/manmeet3/Deep_Learning2/raw/master/Asg4/images_background.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/manmeet3/Deep_Learning2/master/Asg4/images_background.zip [following]\n",
            "--2020-10-18 05:03:00--  https://raw.githubusercontent.com/manmeet3/Deep_Learning2/master/Asg4/images_background.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9464212 (9.0M) [application/zip]\n",
            "Saving to: ‘images_background.zip.1’\n",
            "\n",
            "images_background.z 100%[===================>]   9.03M  46.4MB/s    in 0.2s    \n",
            "\n",
            "2020-10-18 05:03:01 (46.4 MB/s) - ‘images_background.zip.1’ saved [9464212/9464212]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR2Ris-xFUG7",
        "outputId": "71bf31cc-fac7-4b53-c386-099d7bc55f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!wget https://github.com/manmeet3/Deep_Learning2/raw/master/Asg4/images_evaluation.zip"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 05:03:01--  https://github.com/manmeet3/Deep_Learning2/raw/master/Asg4/images_evaluation.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/manmeet3/Deep_Learning2/master/Asg4/images_evaluation.zip [following]\n",
            "--2020-10-18 05:03:01--  https://raw.githubusercontent.com/manmeet3/Deep_Learning2/master/Asg4/images_evaluation.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6462886 (6.2M) [application/zip]\n",
            "Saving to: ‘images_evaluation.zip.1’\n",
            "\n",
            "images_evaluation.z 100%[===================>]   6.16M  30.8MB/s    in 0.2s    \n",
            "\n",
            "2020-10-18 05:03:02 (30.8 MB/s) - ‘images_evaluation.zip.1’ saved [6462886/6462886]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBafSR30FZWN",
        "outputId": "39e73b38-52b1-46f5-e72b-72950554910c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!unzip -q images_background.zip\n",
        "!unzip -q images_evaluation.zip"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace images_background/Alphabet_of_the_Magi/character01/0709_02.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "replace images_evaluation/Angelic/character01/0965_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em5EasOgFgIX",
        "outputId": "681964d4-e9d8-4a74-90a2-e5f09071448a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t       images_background.zip.1\timages_evaluation.zip.1\n",
            "images_background      images_evaluation\tsample_data\n",
            "images_background.zip  images_evaluation.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM7q9qJTFhOG"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imageio import imread # from scipy.misc import imread\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.engine.topology import Layer\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3V2WwhSGxdQ"
      },
      "source": [
        "import numpy.random as rng"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdDtipXpJSsk",
        "outputId": "268fe00d-df1a-4d96-fed5-6c29908272b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPyaz2NlKa1E"
      },
      "source": [
        "# Train contains 30 different languages with 20 instances of \n",
        "# each character within each language\n",
        "train_folder = \"images_background/\"\n",
        "# Alphabets for 20 languages\n",
        "val_folder = \"images_evaluation/\"\n",
        "save_path = \"data/\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlS7cGJOKg2K"
      },
      "source": [
        "def loadimages(path, n=0):\n",
        "  '''\n",
        "  Parameters:\n",
        "  string:path -> Path of train or test directory\n",
        "\n",
        "  Return Value:\n",
        "  tuple:(x, y, language_dictionary)\n",
        "  '''\n",
        "  X=[]\n",
        "  y=[]\n",
        "  cat_dict = {}\n",
        "  lang_dict = {}\n",
        "  curr_y = n\n",
        "\n",
        "  for alphabet in os.listdir(path):\n",
        "    print(\"loading alphabet: \" + alphabet)\n",
        "    lang_dict[alphabet] = [curr_y, None]\n",
        "    alphabet_path = os.path.join(path, alphabet)\n",
        "    # Set each letter as its own column\n",
        "    for letter in os.listdir(alphabet_path):\n",
        "      cat_dict[curr_y] = (alphabet, letter)\n",
        "      category_images=[]\n",
        "      letter_path = os.path.join(alphabet_path, letter)\n",
        "      # read images in current category\n",
        "      for filename in os.listdir(letter_path):\n",
        "        image_path = os.path.join(letter_path, filename)\n",
        "        image = imread(image_path)\n",
        "        category_images.append(image)\n",
        "        y.append(curr_y)\n",
        "      try:\n",
        "        X.append(np.stack(category_images))\n",
        "      # edge case - last one\n",
        "      except ValueError as e:\n",
        "        print(e)\n",
        "        print(\"error - category_images:\", category_images)\n",
        "      curr_y += 1\n",
        "      lang_dict[alphabet][1] = curr_y - 1\n",
        "  y = np.vstack(y)\n",
        "  X = np.stack(X)\n",
        "  return X,y,lang_dict"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDcaYfZrEERP",
        "outputId": "39b67a0c-f602-4b25-f924-9eb38609418e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "X, y, c = loadimages(train_folder)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading alphabet: Hebrew\n",
            "loading alphabet: Greek\n",
            "loading alphabet: Armenian\n",
            "loading alphabet: Mkhedruli_(Georgian)\n",
            "loading alphabet: Latin\n",
            "loading alphabet: Burmese_(Myanmar)\n",
            "loading alphabet: Tagalog\n",
            "loading alphabet: Arcadian\n",
            "loading alphabet: Futurama\n",
            "loading alphabet: Anglo-Saxon_Futhorc\n",
            "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
            "loading alphabet: Braille\n",
            "loading alphabet: Alphabet_of_the_Magi\n",
            "loading alphabet: Early_Aramaic\n",
            "loading alphabet: N_Ko\n",
            "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
            "loading alphabet: Malay_(Jawi_-_Arabic)\n",
            "loading alphabet: Japanese_(katakana)\n",
            "loading alphabet: Tifinagh\n",
            "loading alphabet: Balinese\n",
            "loading alphabet: Gujarati\n",
            "loading alphabet: Asomtavruli_(Georgian)\n",
            "loading alphabet: Bengali\n",
            "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
            "loading alphabet: Grantha\n",
            "loading alphabet: Syriac_(Estrangelo)\n",
            "loading alphabet: Cyrillic\n",
            "loading alphabet: Korean\n",
            "loading alphabet: Japanese_(hiragana)\n",
            "loading alphabet: Sanskrit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpCVku3MFHGD"
      },
      "source": [
        "with open(os.path.join(save_path, \"train.pickle\"), \"wb\") as f:\n",
        "  pickle.dump((X,c ), f)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY4QGet5GMi5",
        "outputId": "a6265f99-6c3d-4cc1-d1f9-7ee6ca250ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "Xval, yval, cval = loadimages(val_folder)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading alphabet: Malayalam\n",
            "loading alphabet: Gurmukhi\n",
            "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
            "loading alphabet: Syriac_(Serto)\n",
            "loading alphabet: Kannada\n",
            "loading alphabet: ULOG\n",
            "loading alphabet: Avesta\n",
            "loading alphabet: Mongolian\n",
            "loading alphabet: Glagolitic\n",
            "loading alphabet: Manipuri\n",
            "loading alphabet: Ge_ez\n",
            "loading alphabet: Sylheti\n",
            "loading alphabet: Atlantean\n",
            "loading alphabet: Keble\n",
            "loading alphabet: Angelic\n",
            "loading alphabet: Tengwar\n",
            "loading alphabet: Tibetan\n",
            "loading alphabet: Aurek-Besh\n",
            "loading alphabet: Oriya\n",
            "loading alphabet: Atemayar_Qelisayer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcH-hpCcGPy-"
      },
      "source": [
        "with open(os.path.join(save_path, \"validate.pickle\"), \"wb\") as f:\n",
        "  pickle.dump((Xval, cval), f)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRC0AZWLHYS-"
      },
      "source": [
        "def initialize_weights(shape, dtype=None):\n",
        "  '''\n",
        "     Initialize CNN layer weights with mean as 0.0 and standard deviation 0f 0.01\n",
        "  '''\n",
        "  return np.random.normal(loc = 0.0, scale = 1e-2, size=shape)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzluwtmJHu4A"
      },
      "source": [
        "def initialize_bias(shape, dtype=None):\n",
        "  '''\n",
        "  Initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
        "  '''\n",
        "  return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB8tvB78lWhO"
      },
      "source": [
        "def get_siamese_model(input_shape):\n",
        "  # Define the tensor for the two input images\n",
        "  left_input = Input(input_shape)\n",
        "  right_input = Input(input_shape)\n",
        "\n",
        "  #Convolutional NN\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
        "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
        "  model.add(MaxPooling2D())\n",
        "  model.add(Conv2D(128, (7,7), activation='relu',\n",
        "                    kernel_initializer=initialize_weights,\n",
        "                    bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "  model.add(MaxPooling2D())\n",
        "  model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
        "                    bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "  model.add(MaxPooling2D())\n",
        "  model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
        "                    bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4096, activation='sigmoid',\n",
        "                  kernel_regularizer=l2(1e-3),\n",
        "                  kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
        "  \n",
        "  # Generate the encodings for the two images\n",
        "  encoded_l = model(left_input)\n",
        "  encoded_r = model(right_input)\n",
        "\n",
        "  # Add a customized layer to compute the absolute difference between encodings\n",
        "  l1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "  l1_distance = l1_layer([encoded_l, encoded_r])\n",
        "\n",
        "  # Add a dense layer with a sigmoid unit to generate similarity score\n",
        "  prediction = Dense(1, activation='sigmoid', bias_initializer=initialize_bias)(l1_distance)\n",
        "  \n",
        "  # Connect the inputs with outputs\n",
        "  siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "\n",
        "  return siamese_net"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-ELHMTgDIEL",
        "outputId": "c21d3da4-6e54-4d5e-9d7b-2cb6ac162e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model = get_siamese_model((105, 105, 1)) #105x105 is the dataset image size\n",
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 4096)         38947648    input_7[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 4096)         0           sequential_3[0][0]               \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 38,951,745\n",
            "Trainable params: 38,951,745\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxh8IG6Q2oKa"
      },
      "source": [
        "optimizer = Adam(lr=0.00006)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBlXndcF20M8",
        "outputId": "ad03eac3-01a6-4d29-9d73-16a3c2a3bdd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "with open(os.path.join(save_path, \"train.pickle\"), \"rb\") as f:\n",
        "  (Xtrain, train_classes) = pickle.load(f)\n",
        "\n",
        "print(\"Training alphabets: \\n\")\n",
        "print(list(train_classes.keys()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training alphabets: \n",
            "\n",
            "['Hebrew', 'Greek', 'Armenian', 'Mkhedruli_(Georgian)', 'Latin', 'Burmese_(Myanmar)', 'Tagalog', 'Arcadian', 'Futurama', 'Anglo-Saxon_Futhorc', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Braille', 'Alphabet_of_the_Magi', 'Early_Aramaic', 'N_Ko', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Malay_(Jawi_-_Arabic)', 'Japanese_(katakana)', 'Tifinagh', 'Balinese', 'Gujarati', 'Asomtavruli_(Georgian)', 'Bengali', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Grantha', 'Syriac_(Estrangelo)', 'Cyrillic', 'Korean', 'Japanese_(hiragana)', 'Sanskrit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeFeGjTk3D2U",
        "outputId": "9c6f421f-892b-4dd8-a7ef-ed835da26189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "with open(os.path.join(save_path, \"validate.pickle\"), \"rb\") as f:\n",
        "  (Xval, val_classes) = pickle.load(f)\n",
        "\n",
        "print(\"Validation alphabets: \\n\")\n",
        "print(list(val_classes.keys()))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation alphabets: \n",
            "\n",
            "['Malayalam', 'Gurmukhi', 'Old_Church_Slavonic_(Cyrillic)', 'Syriac_(Serto)', 'Kannada', 'ULOG', 'Avesta', 'Mongolian', 'Glagolitic', 'Manipuri', 'Ge_ez', 'Sylheti', 'Atlantean', 'Keble', 'Angelic', 'Tengwar', 'Tibetan', 'Aurek-Besh', 'Oriya', 'Atemayar_Qelisayer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi5gS9Rw3R0y"
      },
      "source": [
        "def get_batch(bach_size,s=\"train\"):\n",
        "  ''' Create batch of npairs, half same class, half different class'''\n",
        "  if s == 'train':\n",
        "    X = Xtrain\n",
        "    categories = train_classes\n",
        "  else:\n",
        "    X = Xval\n",
        "    categories = val_classes\n",
        "  n_classes, n_examples, w, h = X.shape\n",
        "\n",
        "  # randomly sample several classes to use in the batch\n",
        "  categories = rng.choice(n_classes, size=(batch_size,), replace=False)\n",
        "\n",
        "  # initialize 2 empty arrays for the input image batch\n",
        "  pairs = [np.zeros((batch_size, h, w, 1)) for i in range(2)]\n",
        "\n",
        "  # initialize vector for the targets\n",
        "  targets=np.zeros((batch_size,))\n",
        "\n",
        "  # Set 2nd half of batch targets to 1\n",
        "  targets[batch_size//2:] = 1\n",
        "  for i in range(batch_size):\n",
        "    category = categories[i]\n",
        "    idx_1 = rng.randint(0, n_examples)\n",
        "    pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
        "    idx_2 = rng.randint(0, n_examples)\n",
        "\n",
        "    # pick images of same class for 1st half, different for 2nd half\n",
        "    if i >= batch_size//2:\n",
        "      category_2 = category_2\n",
        "    else:\n",
        "      # add a random number to the category modulo n classes to ensure 2nd image \n",
        "      # has a different category\n",
        "      category_2 = (category + rng.randint(1, n_classes)) % n_classes\n",
        "    pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h, 1)\n",
        "\n",
        "  return pairs, targets"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjQSmvAH5GxN"
      },
      "source": [
        "def generate(batch_size, s=\"train\"):\n",
        "  ''' A generator for batches so model.fit_generator can be used '''\n",
        "  while True:\n",
        "    pairs, targets = get_batch(batch_size, s)\n",
        "    yield(pairs, targets)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlnNTpcf5WCt"
      },
      "source": [
        "def make_oneshot_task(N, s=\"val\", language=None):\n",
        "  ''' Create pairs of test image, support set for testing N way one-shot learning'''\n",
        "  if s == \"train\":\n",
        "    X = Xtrain\n",
        "    categories = train_classes\n",
        "  else:\n",
        "    X = Xval\n",
        "    categories = val_classes\n",
        "  n_classes, n_examples, w, h = X.shape\n",
        "\n",
        "  indices = rng.randint(0, n_examples, size=(N,))\n",
        "  if language is not None: # select characters if language is defined\n",
        "    low, high = categories[language]\n",
        "    if N > high - low:\n",
        "      raise ValueError(\"This lanaguage ({}) has less than {} letters\".format(language, N))\n",
        "\n",
        "    categories = rng.choice(range(low, high), size=(N,), replace=False)\n",
        "  else:\n",
        "    categories = rng.choice(range(n_classes), size=(N,), replace=False)\n",
        "  true_category = categories[0]\n",
        "  ex1, ex2 = rng.choice(n_examples, replace=False, size=(2,))\n",
        "  test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, 1)\n",
        "  support_set = X[categories, indices, :, :]\n",
        "  support_set[0,:,:] = X[true_category, ex2]\n",
        "  support_set = support_set.reshape(N, w, h, 1)\n",
        "  targets = np.zeros((N,))\n",
        "  targets[0] = 1\n",
        "  targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "  pairs = [test_image, support_set]\n",
        "\n",
        "  return pairs,targets\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWX4J3vW7-Tu"
      },
      "source": [
        "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
        "  ''' Test average N way oneshot learning accuracy of a siamese NN over k one shot tasks '''\n",
        "  n_correct = 0\n",
        "  if verbose:\n",
        "    print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
        "\n",
        "  for i in range(k):\n",
        "    inputs, targets = make_oneshot_task(N,s)\n",
        "    probs = model.predict(inputs)\n",
        "    if np.argmax(probs) == np.argmax(targets):\n",
        "      n_correct += 1\n",
        "  percent_correct = (100.0 * n_correct / k)\n",
        "  if verbose:\n",
        "    print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
        "  return percent_correct"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpthQp_n81UW"
      },
      "source": [
        "\n",
        "# Hyper parameters\n",
        "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
        "batch_size = 32\n",
        "n_iter = 20000 # No. of training iterations\n",
        "N_way = 20 # how many classes for testing one-shot tasks\n",
        "n_val = 250 # how many one-shot tasks to validate on\n",
        "best = -1"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CqKoF5E83Cn",
        "outputId": "04c296cf-3270-4510-c3b6-0b7dfcc5efb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir weights\n",
        "model_path = 'saved_multi_ins_models/'"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z5I0F_DGf5t"
      },
      "source": [
        "!mkdir saved_multi_ins_models"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exnpZYHh8_AZ",
        "outputId": "5d2680ff-2011-4adf-fa6d-1dc6a837b59f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Start training process!\")\n",
        "print(\"----------------------------------\")\n",
        "t_start = time.time()\n",
        "for i in range(1, n_iter+1):\n",
        "  (inputs, targets) = get_batch(batch_size)\n",
        "  loss = model.train_on_batch(inputs, targets)\n",
        "  if i % evaluate_every == 0:\n",
        "    print(\"\\n ----------- \\n\")\n",
        "    print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()- t_start)/60))\n",
        "    print(\"Train_loss: {0}\".format(loss))\n",
        "    val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
        "    model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
        "    if val_acc >= best:\n",
        "      print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
        "      best=val_acc"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training process!\n",
            "----------------------------------\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 200 iterations: 0.2537634611129761 mins\n",
            "Train_loss: 0.7927048802375793\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 16.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 16.4, previous best: -1\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 400 iterations: 0.7037090023358663 mins\n",
            "Train_loss: 0.7818618416786194\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 14.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 600 iterations: 1.1598130583763122 mins\n",
            "Train_loss: 0.7838113307952881\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 800 iterations: 1.61709064245224 mins\n",
            "Train_loss: 0.7521453499794006\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 1000 iterations: 2.067989206314087 mins\n",
            "Train_loss: 0.7615755200386047\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 1200 iterations: 2.5146674513816833 mins\n",
            "Train_loss: 0.7603242993354797\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 16.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 16.8, previous best: 16.4\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 1400 iterations: 2.9615673820177713 mins\n",
            "Train_loss: 0.7435480356216431\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 21.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 21.6, previous best: 16.8\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 1600 iterations: 3.4087159117062886 mins\n",
            "Train_loss: 0.7407674193382263\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 13.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 1800 iterations: 3.857284649213155 mins\n",
            "Train_loss: 0.7601423263549805\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 10.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 2000 iterations: 4.301006976763407 mins\n",
            "Train_loss: 0.7490376234054565\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 2200 iterations: 4.744344607988993 mins\n",
            "Train_loss: 0.7507429122924805\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 14.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 2400 iterations: 5.186924739678701 mins\n",
            "Train_loss: 0.7218132615089417\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 14.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 2600 iterations: 5.630093403657278 mins\n",
            "Train_loss: 0.7209129929542542\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 15.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 2800 iterations: 6.07632797161738 mins\n",
            "Train_loss: 0.7359717488288879\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 15.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 3000 iterations: 6.52187613248825 mins\n",
            "Train_loss: 0.7157167196273804\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 6.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 3200 iterations: 6.968189227581024 mins\n",
            "Train_loss: 0.718201220035553\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 16.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 3400 iterations: 7.416587873299917 mins\n",
            "Train_loss: 0.7222842574119568\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 11.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 3600 iterations: 7.858655107021332 mins\n",
            "Train_loss: 0.7284238934516907\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 14.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 3800 iterations: 8.304306598504384 mins\n",
            "Train_loss: 0.7189232110977173\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 11.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 4000 iterations: 8.746407282352447 mins\n",
            "Train_loss: 0.7129750847816467\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 14.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 4200 iterations: 9.191647740205129 mins\n",
            "Train_loss: 0.7277095317840576\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 15.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 4400 iterations: 9.640129550298054 mins\n",
            "Train_loss: 0.6936604976654053\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 17.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 4600 iterations: 10.08521203994751 mins\n",
            "Train_loss: 0.7155324816703796\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 16.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 4800 iterations: 10.527173352241515 mins\n",
            "Train_loss: 0.7092773914337158\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 14.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 5000 iterations: 10.976771954695383 mins\n",
            "Train_loss: 0.696617066860199\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 16.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 5200 iterations: 11.418440016110738 mins\n",
            "Train_loss: 0.7147740721702576\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 5400 iterations: 11.868524344762166 mins\n",
            "Train_loss: 0.7154727578163147\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 15.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 5600 iterations: 12.313375461101533 mins\n",
            "Train_loss: 0.710206925868988\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 10.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 5800 iterations: 12.760270329316457 mins\n",
            "Train_loss: 0.7102236747741699\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 6000 iterations: 13.20279047091802 mins\n",
            "Train_loss: 0.7118953466415405\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 10.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 6200 iterations: 13.648827580610911 mins\n",
            "Train_loss: 0.7062965631484985\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 10.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 6400 iterations: 14.092519231637318 mins\n",
            "Train_loss: 0.7060396671295166\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 3.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 6600 iterations: 14.542931350072225 mins\n",
            "Train_loss: 0.7061917781829834\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 6.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 6800 iterations: 14.984922234217326 mins\n",
            "Train_loss: 0.7052328586578369\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 6.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 7000 iterations: 15.43308519522349 mins\n",
            "Train_loss: 0.7044432759284973\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 7200 iterations: 15.876885982354482 mins\n",
            "Train_loss: 0.7029379606246948\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 1.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 7400 iterations: 16.32766568660736 mins\n",
            "Train_loss: 0.7021620273590088\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 5.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 7600 iterations: 16.772841278711955 mins\n",
            "Train_loss: 0.7024676203727722\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 7800 iterations: 17.22990390857061 mins\n",
            "Train_loss: 0.7019938230514526\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 14.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 8000 iterations: 17.67563588221868 mins\n",
            "Train_loss: 0.7037038207054138\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 5.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 8200 iterations: 18.126021027565002 mins\n",
            "Train_loss: 0.7016519904136658\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 8.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 8400 iterations: 18.573026311397552 mins\n",
            "Train_loss: 0.7009366750717163\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 15.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 8600 iterations: 19.01986126502355 mins\n",
            "Train_loss: 0.7007399797439575\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 4.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 8800 iterations: 19.465838181972504 mins\n",
            "Train_loss: 0.700647234916687\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 1.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 9000 iterations: 19.922624095280966 mins\n",
            "Train_loss: 0.7003331780433655\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 9.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 9200 iterations: 20.36573383410772 mins\n",
            "Train_loss: 0.6996954083442688\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 18.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 9400 iterations: 20.811861606438956 mins\n",
            "Train_loss: 0.6990654468536377\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 9600 iterations: 21.25837000608444 mins\n",
            "Train_loss: 0.6983396410942078\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 21.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 21.6, previous best: 21.6\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 9800 iterations: 21.708646710713705 mins\n",
            "Train_loss: 0.6979672908782959\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 12.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 10000 iterations: 22.154055281480154 mins\n",
            "Train_loss: 0.6978608965873718\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 10200 iterations: 22.600240349769592 mins\n",
            "Train_loss: 0.6980485916137695\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 10400 iterations: 23.050670417149863 mins\n",
            "Train_loss: 0.6965208649635315\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 25.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 25.2, previous best: 21.6\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 10600 iterations: 23.498252904415132 mins\n",
            "Train_loss: 0.698521614074707\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 10800 iterations: 23.94246503909429 mins\n",
            "Train_loss: 0.6966000199317932\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 20.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 11000 iterations: 24.39423975547155 mins\n",
            "Train_loss: 0.6982608437538147\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 11200 iterations: 24.847856839497883 mins\n",
            "Train_loss: 0.6959030628204346\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 16.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 11400 iterations: 25.30156679948171 mins\n",
            "Train_loss: 0.6963042616844177\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 25.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 25.2, previous best: 25.2\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 11600 iterations: 25.75490183432897 mins\n",
            "Train_loss: 0.7017284631729126\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 17.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 11800 iterations: 26.210866451263428 mins\n",
            "Train_loss: 0.6985059380531311\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 31.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 31.6, previous best: 25.2\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 12000 iterations: 26.664339331785836 mins\n",
            "Train_loss: 0.6978952288627625\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 1.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 12200 iterations: 27.120434685548148 mins\n",
            "Train_loss: 0.6968197822570801\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 24.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 12400 iterations: 27.577372344334922 mins\n",
            "Train_loss: 0.6958836317062378\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 12600 iterations: 28.031869705518087 mins\n",
            "Train_loss: 0.6951974034309387\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 5.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 12800 iterations: 28.48427149852117 mins\n",
            "Train_loss: 0.695446252822876\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 1.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 13000 iterations: 28.938026336828866 mins\n",
            "Train_loss: 0.6984865069389343\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 4.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 13200 iterations: 29.39028468529383 mins\n",
            "Train_loss: 0.6973475217819214\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 13400 iterations: 29.837374564011892 mins\n",
            "Train_loss: 0.696274995803833\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 13600 iterations: 30.285819073518116 mins\n",
            "Train_loss: 0.6958320736885071\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 13800 iterations: 30.734899973869325 mins\n",
            "Train_loss: 0.6955974102020264\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 21.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 14000 iterations: 31.191428057352702 mins\n",
            "Train_loss: 0.6957576870918274\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 6.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 14200 iterations: 31.638311946392058 mins\n",
            "Train_loss: 0.694852352142334\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 20.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 14400 iterations: 32.091681663195295 mins\n",
            "Train_loss: 0.6948585510253906\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 18.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 14600 iterations: 32.545415512720744 mins\n",
            "Train_loss: 0.6942079663276672\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 24.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 14800 iterations: 32.99907570679982 mins\n",
            "Train_loss: 0.6952840685844421\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 15000 iterations: 33.449994242191316 mins\n",
            "Train_loss: 0.6955108046531677\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 15200 iterations: 33.91232646306356 mins\n",
            "Train_loss: 0.694908082485199\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 31.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 31.6, previous best: 31.6\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 15400 iterations: 34.36308246453603 mins\n",
            "Train_loss: 0.6941022276878357\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 25.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 15600 iterations: 34.81900461514791 mins\n",
            "Train_loss: 0.6969510912895203\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 15800 iterations: 35.27491722504298 mins\n",
            "Train_loss: 0.6957616209983826\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 13.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 16000 iterations: 35.72745651801427 mins\n",
            "Train_loss: 0.6936078667640686\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 26.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 16200 iterations: 36.181106968720755 mins\n",
            "Train_loss: 0.695569634437561\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 1.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 16400 iterations: 36.627327700455986 mins\n",
            "Train_loss: 0.6960918307304382\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 16600 iterations: 37.0777454217275 mins\n",
            "Train_loss: 0.6948403716087341\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 16800 iterations: 37.52555466492971 mins\n",
            "Train_loss: 0.6950221061706543\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 17000 iterations: 37.97603391011556 mins\n",
            "Train_loss: 0.7017441391944885\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 13.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 17200 iterations: 38.417888855934144 mins\n",
            "Train_loss: 0.69532310962677\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 9.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 17400 iterations: 38.8614781220754 mins\n",
            "Train_loss: 0.6946632862091064\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 25.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 17600 iterations: 39.31147052049637 mins\n",
            "Train_loss: 0.6989849209785461\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 17800 iterations: 39.75492823123932 mins\n",
            "Train_loss: 0.6977076530456543\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 18000 iterations: 40.200166996320085 mins\n",
            "Train_loss: 0.6974464654922485\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 3.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 18200 iterations: 40.64113448063532 mins\n",
            "Train_loss: 0.6952146291732788\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 32.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 32.8, previous best: 31.6\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 18400 iterations: 41.08406945069631 mins\n",
            "Train_loss: 0.6960476040840149\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 2.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 18600 iterations: 41.521114802360536 mins\n",
            "Train_loss: 0.6943665146827698\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 32.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 18800 iterations: 41.96242595513662 mins\n",
            "Train_loss: 0.6945101618766785\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 19000 iterations: 42.40547649065653 mins\n",
            "Train_loss: 0.6945448517799377\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 19200 iterations: 42.8495793223381 mins\n",
            "Train_loss: 0.6963836550712585\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 15.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 19400 iterations: 43.288293703397116 mins\n",
            "Train_loss: 0.6946934461593628\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 23.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 19600 iterations: 43.7350949883461 mins\n",
            "Train_loss: 0.6917895078659058\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 0.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 19800 iterations: 44.18184792995453 mins\n",
            "Train_loss: 0.6954224109649658\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 19.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "Time for 20000 iterations: 44.62591855923335 mins\n",
            "Train_loss: 0.6942979693412781\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 24.0% 20 way one-shot learning accuracy \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApmudMQXI5eb"
      },
      "source": [
        "model.load_weights(os.path.join(model_path, \"weights.20000.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}